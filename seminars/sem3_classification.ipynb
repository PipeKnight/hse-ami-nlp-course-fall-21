{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "sem3_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6ccN1AlFhNo"
      },
      "source": [
        "## Классификация текста с использованием CNN\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PragmaticsLab/NLP-course-AMI/blob/dev/seminars/sem3_classification.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ8_zAI8FhNp"
      },
      "source": [
        "Используя данные отзывов IMDB, построим CNN для классификации документов на позитивный и негативный классы.\n",
        "\n",
        "Источник изложения: https://github.com/bentrevett/pytorch-sentiment-analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHKLq9hEFhNr"
      },
      "source": [
        "В предположении, что PyTorch уже установлен, поставим дополнительные модули и загрузим модель для токенизации:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgGxeuQjG9NI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4340854c-942d-455d-b342-ae777373f1ad"
      },
      "source": [
        "#!pip3 install https://download.pytorch.org/whl/cpu/torch-1.0.1.post2-cp36-cp36m-linux_x86_64.whl\n",
        "#!pip3 install torch torchvision torchaudio\n",
        "!pip3 install torch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (1.8.0)\n",
            "Requirement already satisfied: torchvision==0.9.0 in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: torchaudio==0.8.0 in /usr/local/lib/python3.7/dist-packages (0.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.0) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kiZro9eG-bG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85e5362c-618e-4e60-cabb-4d3e85a952cb"
      },
      "source": [
        "#!pip install torchvision"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.10.0+cu102)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.9.0+cu102)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchvision) (3.7.4.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-WRgs-VFhNt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c265e48-88de-4a13-855e-693e8187ff7c"
      },
      "source": [
        "!pip install torchtext==0.9.0"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.9.0\n",
            "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (2.23.0)\n",
            "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (1.8.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (4.62.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->torchtext==0.9.0) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (1.24.3)\n",
            "Installing collected packages: torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.8.0\n",
            "    Uninstalling torchtext-0.8.0:\n",
            "      Successfully uninstalled torchtext-0.8.0\n",
            "Successfully installed torchtext-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB7uZ5L7FhN3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b113ef5-b941-4b84-f8d6-51862aa1db1d"
      },
      "source": [
        "!pip install spacy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.6.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLGeXcUjFhN8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab756165-fdcc-4d91-fd2b-68caf226c0ee"
      },
      "source": [
        "#!python3.6 -m spacy download en\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.6.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaPk16PLmgnw"
      },
      "source": [
        "import spacy\n",
        "#import en\n",
        "en_nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLGAJBxfFhN_"
      },
      "source": [
        "Загрузим датасет и получим из него выборку:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsHAXknhFhOA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b79bee06-a08a-447d-cc46-0cba1efa5ea6"
      },
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "#from torchtext import datasets\n",
        "#from torchtext.data.utils import get_tokenizer\n",
        "#from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "print(torch.__version__)\n",
        "\n",
        "SEED = 0\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "\n",
        "#tokenizer = get_tokenizer('spacy')\n",
        "\n",
        "TEXT = data.Field(tokenize='spacy')\n",
        "LABEL = data.LabelField()\n",
        "\n",
        "train_src, test = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.8.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/field.py:150: UserWarning: LabelField class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:08<00:00, 10.5MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJl0ZOPBElSE"
      },
      "source": [
        "Попробуем обучить простую CNN на векторах слов. С учётом того, что в коллекции 100К уникальных слов, и векторы получатся достаточно громоздкие, урежем коллекцию до 25К слов, для всех прочих заведя токен unk (unknown). Кроме того, разобьём обучающий набор данных на собственно обучающий и валидационный набор для настройки параметров."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nuZQIq1FhOl"
      },
      "source": [
        "#print(len(train_src))\n",
        "import random\n",
        "#train, valid = torch.utils.data.random_split(train_src,[20000, 5000], random.seed(SEED))\n",
        "train, valid = train_src.split(random_state=random.seed(SEED))\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsdwfpM5FhOp"
      },
      "source": [
        "#def yield_tokens(data_iter):\n",
        "#    for _, text in data_iter:\n",
        "#        yield tokenizer(text)\n",
        "\n",
        "#vocab = build_vocab_from_iterator(yield_tokens(train_src), specials=[\"<unk>\"])\n",
        "#vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "TEXT.build_vocab(train, max_size=25000)\n",
        "LABEL.build_vocab(train)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAizbRUkXMVt"
      },
      "source": [
        "#text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "#label_pipeline = lambda x: int(x) - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HM9FpclZFhOu"
      },
      "source": [
        "Выше в словаре, помимо отсечения 25К наиболее частых слов, также появятся два специальных токена - unk и pad (padding).\n",
        "\n",
        "Теперь создадим батчи, предварительно отсортировав тексты по длине, чтобы минимизировать вставки pad-токенов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnHqmLZTFhOv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a9e912c-645c-455b-9899-b2a640c710d7"
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train, valid, test), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    sort_key=lambda x: len(x.text), \n",
        "    repeat=False)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf2viKyOE5FF"
      },
      "source": [
        "Опишем функцию подсчёта accuracy, а также функции обучения и применения сети:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQo4PuPzFhPE"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "    rounded_preds = torch.round(F.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jepSRNlkFhPI"
      },
      "source": [
        "def train_func(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(batch.text.cuda()).squeeze(1)\n",
        "\n",
        "        loss = criterion(predictions.float(), batch.label.float().cuda())\n",
        "        acc = binary_accuracy(predictions.float(), batch.label.float().cuda())\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss\n",
        "        epoch_acc += acc\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nb0KPBl8FhPL"
      },
      "source": [
        "def evaluate_func(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            predictions = model(batch.text.cuda()).squeeze(1)\n",
        "\n",
        "            loss = criterion(predictions.float(), batch.label.float().cuda())\n",
        "            acc = binary_accuracy(predictions.float(), batch.label.float().cuda())\n",
        "\n",
        "            epoch_loss += loss\n",
        "            epoch_acc += acc\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm187c4CFhPX"
      },
      "source": [
        "Начнём с подготовки данных:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "By88bjwfFhPX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a77f91ef-4e78-4421-ccee-50ea3f67346d"
      },
      "source": [
        "TEXT.build_vocab(train, max_size=25000, vectors=\"glove.6B.100d\")\n",
        "LABEL.build_vocab(train)\n",
        "\n",
        "\n",
        "BATCH_SIZEBATCH_S  = 8\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train, valid, test), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    sort_key=lambda x: len(x.text), \n",
        "    repeat=False)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:42, 5.31MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:21<00:00, 18591.16it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHEDOjrAFhP5"
      },
      "source": [
        "Для создания свёрточного слоя воспользуемся nn.Conv2d, in_channels в нашем случае один (текст), out_channels -- это число фильтров и размер ядер всех фильтров. Каждый фильтр будет иметь размерность [n x размерность эмбеддинга], где n - размер обрабатываемой n-граммы.\n",
        "\n",
        "Важно, чтобы предложения имели длину не меньше размера самого большого из используемых фильтров (здесь это не страшно, поскольку в используемых данных нет текстов, состоящих из пяти и менее слов)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb5g3czaFhP6"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.conv_0 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[0], embedding_dim))\n",
        "        self.conv_1 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[1], embedding_dim))\n",
        "        self.conv_2 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[2], embedding_dim))\n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        #x = [sent len, batch size]\n",
        "        x = x.permute(1, 0)\n",
        "                \n",
        "        #x = [batch size, sent len]\n",
        "        embedded = self.embedding(x)\n",
        "                \n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        \n",
        "        #embedded = [batch size, 1, sent len, emb dim]\n",
        "        conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\n",
        "        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n",
        "        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\n",
        "            \n",
        "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
        "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
        "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
        "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
        "        \n",
        "        #pooled_n = [batch size, n_filters]\n",
        "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n",
        "\n",
        "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
        "        return self.fc(cat)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_xPva9zFhP8"
      },
      "source": [
        "Сейчас мы можем использовать только три различных фильтра, хотелось бы больше. Воспользуйтесь nn.ModuleList и перепишите класс сети для того, чтобы фильтров создавалось по количеству элементов в filter_sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG4nFyJaqESM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eWasooHFhQA"
      },
      "source": [
        "Cоздадим модель и загрузим предобученные эмбеддинги:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Q_Yvs_gFhQB"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "N_FILTERS = 100\n",
        "FILTER_SIZES = [3,4,5]\n",
        "OUTPUT_DIM = 1\n",
        "DROPOUT = 0.5\n",
        "\n",
        "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWlPRIqHFhQD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31eaea89-b4b9-4dfa-a424-f36e5938e490"
      },
      "source": [
        "TEXT.build_vocab(train, max_size=25000, vectors=\"glove.6B.100d\")\n",
        "LABEL.build_vocab(train)\n",
        "\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [ 0.0373, -0.2515,  0.4574,  ...,  0.0701, -0.7146,  0.1973],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [-0.3347,  0.5661,  0.2080,  ..., -0.3629, -0.2410,  0.7844]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnFgj1oRGl4k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6796d393-b85c-4744-83c3-afeea4281b8b"
      },
      "source": [
        "BATCH_SIZE  = 64\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train, valid, test), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    sort_key=lambda x: len(x.text), \n",
        "    repeat=False)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmWIaqIbFhQF"
      },
      "source": [
        "Используя определённые ранее функции, запустим обучение с оптимизатором Adam и оценим качество на валидации и тесте:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6g2pMt9rX2p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a240c134-96e0-444e-8975-449b1ea9b3e8"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Sep 15 22:00:56 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   72C    P8    34W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np-BTnydFhQF"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = model.cuda()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZC7S33pFhQH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "492eb1ab-1489-48e4-c1af-c036890d8cc6"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss, train_acc = train_func(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate_func(model, valid_iterator, criterion)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Train Loss: 0.502, Train Acc: 74.31%, Val. Loss: 0.347, Val. Acc: 84.61%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXQYQCLCFhQJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "8de18b2a-b63d-4ba9-af6d-9cea9b34b934"
      },
      "source": [
        "test_loss , test_acc = evaluate_func(model, test_iterator, criterion)\n",
        "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.308, Test Acc: 88.25%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sfuamkw28rgp"
      },
      "source": [
        "# Аугментация данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89zDRG-jIdNB"
      },
      "source": [
        "В нашем примере данные были сбалансированными. А как работать с несбалансированными данными?\n",
        "\n",
        "Рассмотрим задачу распознавания тональности твитов, взятых из [Twitter Sentimental Analysis challenge](https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/).\n",
        "\n",
        "Источник изложения: https://github.com/mabusalah/Resampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLKzWFu-QWAm"
      },
      "source": [
        "Получим данные"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_WpbqWhk-a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2acdce9d-9d7d-4335-fdb8-cfefc6fb24b2"
      },
      "source": [
        "!wget https://www.dropbox.com/s/3eefrxvfin11m5p/test_tweets.csv\n",
        "!wget https://www.dropbox.com/s/rzpc5gxlvl6zh8a/train.csv"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-15 21:04:30--  https://www.dropbox.com/s/3eefrxvfin11m5p/test_tweets.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.81.18, 2620:100:6035:18::a27d:5512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.81.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/3eefrxvfin11m5p/test_tweets.csv [following]\n",
            "--2021-09-15 21:04:30--  https://www.dropbox.com/s/raw/3eefrxvfin11m5p/test_tweets.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc8aa321905128aa64e0fa786b83.dl.dropboxusercontent.com/cd/0/inline/BWN3n6FWeO3wXk7CpLGmyYptIsQL66Z2CL98FUFS4CFmt31UnOBEzriMYXE4pbMrBpM_jXS4qGqJBbKBxegsbDJGPCll8OJ_tMEud5O_AonQEVxfV8Gw2ztZcERE3XzUOcg6V6NxUSPvCLcJ-LQUJ5zV/file# [following]\n",
            "--2021-09-15 21:04:31--  https://uc8aa321905128aa64e0fa786b83.dl.dropboxusercontent.com/cd/0/inline/BWN3n6FWeO3wXk7CpLGmyYptIsQL66Z2CL98FUFS4CFmt31UnOBEzriMYXE4pbMrBpM_jXS4qGqJBbKBxegsbDJGPCll8OJ_tMEud5O_AonQEVxfV8Gw2ztZcERE3XzUOcg6V6NxUSPvCLcJ-LQUJ5zV/file\n",
            "Resolving uc8aa321905128aa64e0fa786b83.dl.dropboxusercontent.com (uc8aa321905128aa64e0fa786b83.dl.dropboxusercontent.com)... 162.125.80.15, 2620:100:6030:15::a27d:500f\n",
            "Connecting to uc8aa321905128aa64e0fa786b83.dl.dropboxusercontent.com (uc8aa321905128aa64e0fa786b83.dl.dropboxusercontent.com)|162.125.80.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1635543 (1.6M) [text/plain]\n",
            "Saving to: ‘test_tweets.csv’\n",
            "\n",
            "test_tweets.csv     100%[===================>]   1.56M  1.81MB/s    in 0.9s    \n",
            "\n",
            "2021-09-15 21:04:33 (1.81 MB/s) - ‘test_tweets.csv’ saved [1635543/1635543]\n",
            "\n",
            "--2021-09-15 21:04:33--  https://www.dropbox.com/s/rzpc5gxlvl6zh8a/train.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.81.18, 2620:100:6035:18::a27d:5512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.81.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/rzpc5gxlvl6zh8a/train.csv [following]\n",
            "--2021-09-15 21:04:33--  https://www.dropbox.com/s/raw/rzpc5gxlvl6zh8a/train.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc853f18de70bfd90e41e7453c34.dl.dropboxusercontent.com/cd/0/inline/BWN23ikeeE8NS3Kfbor8masrO--8SImhHP-7ZAljyKQKnqtNKQitRHyMea5HCAGZiG-LH5kuFoT0KVR1nOKhXPHuwsRByBR4f0ybF6i2f4Iqo_j9AsO6vaGkokMxpNK_e80mOJtk1kbnbNTGPc4NEya6/file# [following]\n",
            "--2021-09-15 21:04:34--  https://uc853f18de70bfd90e41e7453c34.dl.dropboxusercontent.com/cd/0/inline/BWN23ikeeE8NS3Kfbor8masrO--8SImhHP-7ZAljyKQKnqtNKQitRHyMea5HCAGZiG-LH5kuFoT0KVR1nOKhXPHuwsRByBR4f0ybF6i2f4Iqo_j9AsO6vaGkokMxpNK_e80mOJtk1kbnbNTGPc4NEya6/file\n",
            "Resolving uc853f18de70bfd90e41e7453c34.dl.dropboxusercontent.com (uc853f18de70bfd90e41e7453c34.dl.dropboxusercontent.com)... 162.125.80.15, 2620:100:6030:15::a27d:500f\n",
            "Connecting to uc853f18de70bfd90e41e7453c34.dl.dropboxusercontent.com (uc853f18de70bfd90e41e7453c34.dl.dropboxusercontent.com)|162.125.80.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3103165 (3.0M) [text/plain]\n",
            "Saving to: ‘train.csv’\n",
            "\n",
            "train.csv           100%[===================>]   2.96M  3.01MB/s    in 1.0s    \n",
            "\n",
            "2021-09-15 21:04:36 (3.01 MB/s) - ‘train.csv’ saved [3103165/3103165]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILRsSOuWJGFP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cc32284-70e0-4691-adce-3577bd450602"
      },
      "source": [
        "import pandas as pd\n",
        "test = pd.read_csv('test_tweets.csv')\n",
        "print(\"Test Set:\"% test.columns, test.shape, len(test))\n",
        "train = pd.read_csv('train.csv')\n",
        "print(\"Training Set:\"% train.columns, train.shape, len(train))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Set: (17197, 2) 17197\n",
            "Training Set: (31962, 3) 31962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBtAsQDNJPXX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "e098898b-d6f5-4343-9b54-8ee0c4d9c0bc"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>@user when a father is dysfunctional and is s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>bihday your majesty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>#model   i love u take with u all the time in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>factsguide: society now    #motivation</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  label                                              tweet\n",
              "0   1      0   @user when a father is dysfunctional and is s...\n",
              "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
              "2   3      0                                bihday your majesty\n",
              "3   4      0  #model   i love u take with u all the time in ...\n",
              "4   5      0             factsguide: society now    #motivation"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV-4lGQVJRc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "95dda4c5-6878-4015-bc35-fc4b5606f582"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>31963</td>\n",
              "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>31964</td>\n",
              "      <td>@user #white #supremacists want everyone to s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>31965</td>\n",
              "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>31966</td>\n",
              "      <td>is the hp and the cursed child book up for res...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>31967</td>\n",
              "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id                                              tweet\n",
              "0  31963  #studiolife #aislife #requires #passion #dedic...\n",
              "1  31964   @user #white #supremacists want everyone to s...\n",
              "2  31965  safe ways to heal your #acne!!    #altwaystohe...\n",
              "3  31966  is the hp and the cursed child book up for res...\n",
              "4  31967    3rd #bihday to my amazing, hilarious #nephew..."
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xarXsO4DJU8Y"
      },
      "source": [
        "Итак, посмотрим, какой процент от общей выборки занимают позитивные и негативные примеры."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT7Rz3MNJTVP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a8afcbf-1ed8-4d99-b52f-87ac632ad353"
      },
      "source": [
        "print(\"Positive: \", train.label.value_counts()[0]/len(train)*100,\"%\")\n",
        "print(\"Negative: \", train.label.value_counts()[1]/len(train)*100,\"%\")"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive:  92.98542018647143 %\n",
            "Negative:  7.014579813528565 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB6Jaz-mJ1zR"
      },
      "source": [
        "93% vs. 7% - данные определенно несбалансированны, что, в свою очередь, негативно влияет на точность предсказания.\n",
        "Для начала поработаем с исходными данными и оценим точность классификации.\n",
        "Начнем с предобработки данных: уберем из твитов числа, html/xml-тэги, специальные символы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCmZHO1kJfpP"
      },
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup #для работы с html/xml-тэгами\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "porter=PorterStemmer()\n",
        "tok = WordPunctTokenizer()\n",
        "pat1 = r'@[A-Za-z0-9]+'\n",
        "pat2 = r'https?://[A-Za-z0-9./]+'\n",
        "combined_pat = r'|'.join((pat1, pat2))\n",
        "\n",
        "def tweet_cleaner(text):\n",
        "    soup = BeautifulSoup(text, 'lxml')\n",
        "    souped = soup.get_text()\n",
        "    stripped = re.sub(combined_pat, '', souped)\n",
        "    try:\n",
        "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
        "    except:\n",
        "        clean = stripped\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
        "    lower_case = letters_only.lower()\n",
        "\n",
        "    words = tok.tokenize(lower_case)\n",
        "    \n",
        "    stem_sentence=[]\n",
        "    for word in words:\n",
        "        stem_sentence.append(porter.stem(word))\n",
        "        stem_sentence.append(\" \")\n",
        "    words=\"\".join(stem_sentence).strip()\n",
        "    return words\n",
        "\n",
        "nums = [0,len(train)]\n",
        "clean_tweet_texts = []\n",
        "for i in range(nums[0],nums[1]):\n",
        "    clean_tweet_texts.append(tweet_cleaner(train['tweet'][i]))\n",
        "    \n",
        "nums = [0,len(test)]\n",
        "test_tweet_texts = []\n",
        "\n",
        "for i in range(nums[0],nums[1]):\n",
        "    test_tweet_texts.append(tweet_cleaner(test['tweet'][i])) \n",
        "    \n",
        "train_clean = pd.DataFrame(clean_tweet_texts,columns=['tweet'])\n",
        "train_clean['label'] = train.label\n",
        "train_clean['id'] = train.id\n",
        "test_clean = pd.DataFrame(test_tweet_texts,columns=['tweet'])\n",
        "test_clean['id'] = test.id"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkQ5fDVUM9EU"
      },
      "source": [
        "Разделим данные на обучающие и проверочные."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d23SW3-tMdKk"
      },
      "source": [
        "from sklearn import model_selection, preprocessing, metrics, linear_model, svm\n",
        "\n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train_clean['tweet'],train_clean['label'])\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "valid_y = encoder.fit_transform(valid_y)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tfj1ZjQANVSK"
      },
      "source": [
        "Рассчитаем TF-IDF признаки."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNw9U1WnNKfq"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=100000)\n",
        "tfidf_vect.fit(train_clean['tweet'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
        "xvalid_tfidf =  tfidf_vect.transform(valid_x)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIaiuRviW5pS"
      },
      "source": [
        "Попробуйте использовать обычный счетчик слов для извлечения признаков."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vsr0wT4NW2LW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8TxxultNlOq"
      },
      "source": [
        "Точность в качестве метрики работает хорошо только на сбалансированных наборах данных, поэтому для оценки результатов работы  алгоритма будем использовать F1-метрику."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx29ZstgNXqb"
      },
      "source": [
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid):\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "\n",
        "    predictions = classifier.predict(feature_vector_valid)    \n",
        "\n",
        "    return metrics.f1_score(valid_y,predictions)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YEjiEUDNrLj"
      },
      "source": [
        "Для начала обучим лог-регрессию."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG3DifE-Nn-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "171df383-1e71-4dd8-9e3a-a7271cec26e5"
      },
      "source": [
        "accuracyORIGINAL = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print (\"Logistic regression Baseline, WordLevel TFIDF: \", accuracyORIGINAL)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression Baseline, WordLevel TFIDF:  0.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0baBnaYN7a0"
      },
      "source": [
        "Как видно, результат оставляет желать лучшего.\n",
        "\n",
        "Что можно сделать с данными?\n",
        "\n",
        "Было бы неплохо как-то увеличить  количество негативных примеров, или же уменьшить количество положительных. Для этого существуют различные техники аугментации данных. \n",
        "В Python для этих целей есть библиотека imblearn (imbalanced-learn)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6zNqmnUN1WB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "244444e9-5737-436b-e48d-42c33f753af1"
      },
      "source": [
        "from imblearn.over_sampling import BorderlineSMOTE, SMOTE, ADASYN, SMOTENC, RandomOverSampler\n",
        "from imblearn.under_sampling import (RandomUnderSampler, \n",
        "                                    NearMiss, \n",
        "                                    InstanceHardnessThreshold,\n",
        "                                    CondensedNearestNeighbour,\n",
        "                                    EditedNearestNeighbours,\n",
        "                                    RepeatedEditedNearestNeighbours,\n",
        "                                    AllKNN,\n",
        "                                    NeighbourhoodCleaningRule,\n",
        "                                    OneSidedSelection,\n",
        "                                    TomekLinks)\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from imblearn.pipeline import make_pipeline"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efoIi6euYMQs"
      },
      "source": [
        "Итак, в качестве инструментов для аугментации рассмотрим: under-sampling, over-sampling и их комбинацию."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqhD2dodYwJl"
      },
      "source": [
        "**Under-sampling** уравновешивает данные за счет уменьшения размера  превалирующего класса.\n",
        "Этот метод разумно использовать, когда количество данных достаточно велико, иначе есть риск остаться и вовсе без обучающих примеров.\n",
        "\n",
        "Итак, логика действия довольно проста: мы просто случайным образом убираем лишние экземпляры из превалирующего класса.\n",
        "\n",
        "Так как в нашем примере лишь 7% всех твитов имеют негативную окраску, уравновешивание позитивного набора с этими 7-ю процентами вряд ли обеспечит хороший результат.\n",
        "\n",
        "Попробуем..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQVfIvf0aTrp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14dec5f5-75d7-433d-89ae-738d6cc07fd1"
      },
      "source": [
        "rus = RandomUnderSampler(random_state=0, replacement=True)\n",
        "rus_xtrain_tfidf, rus_train_y = rus.fit_sample(xtrain_tfidf, train_y)\n",
        "accuracyrus = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),rus_xtrain_tfidf, rus_train_y, xvalid_tfidf)\n",
        "print (\"Logistic regressio RUS, WordLevel TFIDF: \", accuracyrus)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regressio RUS, WordLevel TFIDF:  0.4890438247011952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnwqxfGeaa9R"
      },
      "source": [
        "Действительно, все стало только хуже.\n",
        "\n",
        "Попробуем другие алгоритмы **under-sampling**.\n",
        "\n",
        "Например, **NearMiss**. Данный алгоритм выбирает, какие экземпляры нужно оставить в превалирующем классе на основании некоторых эвристик. Существует три варианта данного алгоритма:\n",
        "\n",
        "**NearMiss-1** оставляет те экземпляры из превалирующего класса, для которых среднее расстояние до *k* ближайших соседей из миноритарного класса будет наименьшим.\n",
        "\n",
        "**NearMiss-2** оставляет те экземпляры из превалирующего класса, для которых среднее расстояние до *k* самых дальних соседей из миноритарного класса будет наименьшим.\n",
        "\n",
        "**NearMiss-3** состоит из двух шагов: сначала, для каждого экземпляра из миноритарного класса выбирается *k* ближайших соседей из превалирующего класса, затем из большего класса выбираются те экземпляры, для которых среднее расстояние до *k* ближайших соседей максимальное."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSlgmHmOd9tU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef5f6002-1dd9-474e-ad5d-5dcad6b24af8"
      },
      "source": [
        "for sampler in (NearMiss(version=1),NearMiss(version=2),NearMiss(version=3)):\n",
        "    nm_xtrain_tfidf, nm_train_y = sampler.fit_sample(xtrain_tfidf, train_y)\n",
        "    accuracysm = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),nm_xtrain_tfidf, nm_train_y, xvalid_tfidf)\n",
        "    print (\"Logistic regression NearMiss(version= {0}), WordLevel TFIDF: \".format(sampler.version), accuracysm)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression NearMiss(version= 1), WordLevel TFIDF:  0.25327291037260824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression NearMiss(version= 2), WordLevel TFIDF:  0.49974186886938565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/imblearn/under_sampling/_prototype_selection/_nearmiss.py:194: UserWarning: The number of the samples to be selected is larger than the number of samples available. The balancing ratio cannot be ensure and all samples will be returned.\n",
            "  warnings.warn('The number of the samples to be selected is larger'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression NearMiss(version= 3), WordLevel TFIDF:  0.3022041763341068\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9nNg2irexcc"
      },
      "source": [
        "**Edited Nearest Neighbor (ENN)**\n",
        "\n",
        "ENN удаляет из большего класса элемент, если класс его ближайшего соседа отличается от его собственного."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFjd6PzjfWEU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e8d64d3-2600-4789-dcf1-4b80d0e163bd"
      },
      "source": [
        "enn_xtrain_tfidf, enn_train_y = EditedNearestNeighbours().fit_sample(xtrain_tfidf, train_y)\n",
        "accuracy = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),enn_xtrain_tfidf, enn_train_y, xvalid_tfidf)\n",
        "print (\"Logistic regression {0}, WordLevel TFIDF: \", accuracy)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression {0}, WordLevel TFIDF:  0.568019093078759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VcHHV4jgZ50"
      },
      "source": [
        "Как вы поняли, при применении **Under-samplin**g техник новые данные не генерируются, в отличие от **Over-sampling**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CHYj7jW7z5q"
      },
      "source": [
        "# Over-sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODhcR4G672lq"
      },
      "source": [
        "Итак, когда данных недостаточно или количество экземпляров в миноритарном классе очень мало, применяется **Over-sampling**. \n",
        "\n",
        "При применении этой техники балансировка данных происходит за счет увеличения количества экземпляров в миноритарном классе. Новые элементы генерируются за счет: повторения, бутстрэппинга, SMOTE (Synthetic Minority Over-Sampling Technique) или ADASYN (Adaptive synthetic sampling)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ea9aYY4mxsqo"
      },
      "source": [
        "**Random Over-sampling**: случайным образом дублируются некоторые элементы из миноритарного класса."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-6Ivm0ZOCOt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "573c67f6-e9b2-4630-b4a8-923128178f77"
      },
      "source": [
        "#Random Over Sampling\n",
        "ros = RandomOverSampler(random_state=777)\n",
        "ros_xtrain_tfidf, ros_train_y = ros.fit_sample(xtrain_tfidf, train_y)\n",
        "accuracyROS = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),ros_xtrain_tfidf, ros_train_y, xvalid_tfidf)\n",
        "print (\"Logistic regression ROS, WordLevel TFIDF: \", accuracyROS)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression ROS, WordLevel TFIDF:  0.6774441878367975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXp1gpdDz5p5"
      },
      "source": [
        "**SMOTE Over-sampling**\n",
        "\n",
        "Алгоритм SMOTE основан на идее генерации некоторого количества искусственных примеров, которые были бы «похожи» на имеющиеся в миноритарном классе, но при этом не дублировали их.\n",
        "\n",
        "Для создания новой записи находят разность $d=X_b-X_a,$ где $ X_b, X_a -$ векторы признаков «соседних» примеров $a$ и $b$ из миноритарного класса. \n",
        "\n",
        "Их находят, используя алгоритм ближайшего соседа (*KNN*). В данном случае необходимо и достаточно для примера $b$ получить набор из $k$ соседей, из которого в дальнейшем будет выбрана запись $b$. Остальные шаги алгоритма *KNN* не требуются.\n",
        "\n",
        "Далее из $d$ путем умножения каждого его элемента на случайное число в интервале (0, 1) получают $\\hat{d}$. Вектор признаков нового примера вычисляется путем сложения $X_a$ и $\\hat{d}$. \n",
        "\n",
        "Алгоритм **SMOTE** позволяет задавать количество записей, которое необходимо искусственно сгенерировать. Степень сходства примеров $a$ и $b$ можно регулировать путем изменения значения $k$ (числа ближайших соседей)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWzqbIWsOEg1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa1fc8bc-57ad-48fb-c379-2daca71f7f9c"
      },
      "source": [
        "sm = SMOTE(random_state=777, ratio = 1.0)\n",
        "sm_xtrain_tfidf, sm_train_y = sm.fit_sample(xtrain_tfidf, train_y)\n",
        "accuracySMOTE = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),sm_xtrain_tfidf, sm_train_y, xvalid_tfidf)\n",
        "print (\"Logistic regression SMOTE, WordLevel TFIDF: \", accuracySMOTE)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression SMOTE, WordLevel TFIDF:  0.6835443037974684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gpJkvdb10Sg"
      },
      "source": [
        "Итак, по сравнению с **Random Over-sampling** разница небольшая.\n",
        "\n",
        "Проверьте результаты **Random Over-sampling** и **SMOTE Over-sampling** для реальных тестовых данных (*test_clean*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7V-VoCAO7qw0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Jz0o7k82YoX"
      },
      "source": [
        "Следующий алгоритм **ASMO: Adaptive synthetic minority oversampling**.\n",
        "\n",
        "\n",
        "\n",
        "Сгенерировать искусственные записи в пределах отдельных кластеров на основе всех классов. Для каждого примера миноритарного класса находят m ближайших соседей, и на основе них (также как в SMOTE) создаются новые записи.\n",
        "\n",
        "1.   Если для каждого $i$-ого примера миноритарного класса из $k$ ближайших соседей $g$ ($g\\leq k$) принадлежит к мажоритарному, то набор данных считается «рассеянным». В этом случае используют алгоритм **ASMO**, иначе применяют **SMOTE** (как правило, $g$ задают равным 20).\n",
        "2.   Используя только примеры миноритарного класса, выделить несколько кластеров (например, алгоритмом $k$-means).\n",
        "3.   Сгенерировать искусственные записи в пределах отдельных кластеров на основе всех классов. Для каждого примера миноритарного класса находят m ближайших соседей, и на основе них (также как в **SMOTE**) создаются новые записи.\n",
        "\n",
        "Такая модификация алгоритма **SMOTE** делает его более адаптивным к различным наборам данных с несбалансированными классами."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mClRDnFM1weo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2433739-d776-41f2-8c5c-74b6f66db9c0"
      },
      "source": [
        "ad = ADASYN(random_state=777, ratio = 1.0)\n",
        "ad_xtrain_tfidf, ad_train_y = ad.fit_sample(xtrain_tfidf, train_y)\n",
        "accuracyADASYN = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),ad_xtrain_tfidf, ad_train_y, xvalid_tfidf)\n",
        "print (\"Logistic regression ADASYN, WordLevel TFIDF: \", accuracyADASYN)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression ADASYN, WordLevel TFIDF:  0.677992277992278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHRcvIRP4fx7"
      },
      "source": [
        "И опять проверим на реальных тестовых примерах."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTzMbjvnRyFD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "br9FjH077ix0"
      },
      "source": [
        "# Комбинация **Under-** и **Over-sampling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-RCI42f42Hi"
      },
      "source": [
        "В *imblearn* реализованы две возможные комбинации:\n",
        "\n",
        "\n",
        "1.   **SMOTE** + **ENN**\n",
        "2.   **SMOTE** + **Tomek Link Removal** (Пара двух ближайших соседей, которые принадлежат разным классам называется *Tomek link*. Under-sampling заключается в удалении всех таких элементов из мажоритарного класса)\n",
        "\n",
        "Подробнее: https://imbalanced-learn.readthedocs.io/en/stable/api.html#module-imblearn.combine\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pg1YBvT4-xP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdd0f268-ddc3-44f1-a0cf-7a3303e9a83e"
      },
      "source": [
        "se = SMOTEENN(random_state=42)\n",
        "se_xtrain_tfidf, se_train_y = se.fit_sample(xtrain_tfidf, train_y)\n",
        "accuracy = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),se_xtrain_tfidf, se_train_y, xvalid_tfidf)\n",
        "print (\"Logistic regression SMOTEENN: \", accuracy)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression SMOTEENN:  0.5094629156010231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Gd9dCEd5aDs"
      },
      "source": [
        "Первый метод сработал плохо. Оцените работу второго подхода."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkZofkXo5RG2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "a95a143f-d241-4d82-bc52-18cab5eb6baa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Logistic regression SMOTEENN:  0.6945337620578779\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}